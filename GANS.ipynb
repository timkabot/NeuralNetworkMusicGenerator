{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "GANS.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNVG0sa5CqoXEVB1RxDmuOg",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/timkabot/NeuralNetworkMusicGenerator/blob/master/GANS.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cwJOkkQUEhOn",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 80
        },
        "outputId": "d484107b-7121-4f47-cd5a-2fbc43813bfa"
      },
      "source": [
        "  print(\"Tensorflow version is\", tf.__version__)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<p style=\"color: red;\">\n",
              "The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x.<br>\n",
              "We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now \n",
              "or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n",
              "<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Tensorflow version is 1.15.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cRIG0RHt26Br"
      },
      "source": [
        "class CGAN():\n",
        "  def __init__(self, embeddings, grouped_embeddings, sample_interval, plot_interval):\n",
        "    self.generator_losses = []\n",
        "    self.discriminator_losses = []\n",
        "    self.loss_saving_rate = 1\n",
        "    self.loss_plot_rate = plot_interval\n",
        "    self.distribution_plot_rate = sample_interval\n",
        "\n",
        "    self.all_normalized_lyrics_embeddings = embeddings\n",
        "    self.grouped_embeddings = grouped_embeddings\n",
        "\n",
        "    self.song_size = 20\n",
        "    self.noise_size = 30\n",
        "    self.label_size = 20\n",
        "    self.optimizer = Adam(lr=0.0002, beta_1=0.5)\n",
        "\n",
        "    self.discriminator = self.build_discriminator()\n",
        "    self.discriminator.compile(loss=['binary_crossentropy'],\n",
        "            optimizer=self.optimizer,\n",
        "            metrics=['accuracy'])\n",
        "    self.generator = self.build_generator()\n",
        "\n",
        "    noise = Input(shape = (self.song_size,  self.noise_size), name=\"noise\")\n",
        "    labels = Input(shape = (self.song_size, self.label_size), name=\"labels\")\n",
        "    generated_midi_sequence = self.generator([noise, labels])\n",
        "\n",
        "    # For the combined model we will only train the generator\n",
        "    self.discriminator.trainable = False\n",
        "    valid = self.discriminator([generated_midi_sequence, labels])\n",
        "\n",
        "    # The combined model  (stacked generator and discriminator)\n",
        "    # Trains generator to fool discriminator\n",
        "    self.combined = keras.Model([noise,labels], valid, name='Combined')\n",
        "    self.combined.compile(loss=['binary_crossentropy'],  # BCE(D(G(Z)),1))\n",
        "        optimizer=self.optimizer)\n",
        "\n",
        "  def build_generator(self):\n",
        "    model = keras.Sequential( [\n",
        "    Dense(400, input_shape=(self.song_size, self.noise_size + 20)),\n",
        "    LeakyReLU(alpha=0.2),\n",
        "    LSTM(400, return_sequences=True, activation = \"tanh\"),    \n",
        "    Dropout(0.25),\n",
        "    LSTM(400, return_sequences=True, activation = \"tanh\"),\n",
        "    Dense(3, activation = \"tanh\")\n",
        "    ])\n",
        "\n",
        "    model.build((self.song_size, self.noise_size + 20))\n",
        "    model.summary()\n",
        "    \n",
        "    noise  = Input(shape=(self.song_size,  self.noise_size), name='z_input')\n",
        "    labels = Input(shape=(self.song_size, self.label_size), name='class_labels')\n",
        "    \n",
        "    generator_input = Concatenate()([noise, labels])\n",
        "    outputs = model(generator_input)\n",
        "  \n",
        "    return keras.Model([noise, labels], outputs, name='generator')\n",
        "\n",
        "  def build_discriminator(self):\n",
        "    model = keras.Sequential([\n",
        "    LSTM(400, return_sequences=True, input_shape=(self.song_size, self.label_size + 3), activation = \"tanh\"),\n",
        "    Dropout(0.25),\n",
        "    LSTM(400, activation=\"tanh\"),\n",
        "    Flatten(),\n",
        "    Dense(1, activation='sigmoid')])\n",
        "        \n",
        "    model.build((self.song_size, self.label_size + 3))\n",
        "    model.summary()\n",
        "\n",
        "    notes  = Input(shape=(self.song_size, 3), name=\"notes\")\n",
        "    labels = Input(shape=(self.song_size, self.label_size), name=\"labels\")\n",
        "    model_input = Concatenate()([notes, labels])\n",
        "\n",
        "    validity = model(model_input)\n",
        "    \n",
        "    return keras.Model([notes,labels], outputs=validity, name='discriminator')\n",
        "\n",
        "  def save_models(self, epoch):\n",
        "    self.generator.save_weights(\"/content/drive/My Drive/Thesis/SecondSemester/CGAN/generator_epoch_%d.h5\" % epoch)\n",
        "    self.discriminator.save_weights(\"/content/drive/My Drive/Thesis/SecondSemester/CGAN/discriminator_epoch_%d.h5\" % epoch)\n",
        "    self.combined.save_weights(\"/content/drive/My Drive/Thesis/SecondSemester/CGAN/combined_epoch_%d.h5\" % epoch)\n",
        "\n",
        "  def train(self, epochs, X_train, y_train,song_size, batch_size=1):\n",
        "    valid = np.ones((batch_size, 1))  # Adversarial ground truths\n",
        "    fake = np.zeros((batch_size, 1))  # Fake labels\n",
        "    for epoch in tqdm_notebook(range(1, epochs)):\n",
        "      # Train DISCRIMINATOR\n",
        "      song_number = np.random.randint(0, len(X_train), batch_size)\n",
        "      notes, labels = X_train[song_number], y_train[song_number]\n",
        "\n",
        "      notes = notes.reshape(batch_size, song_size, 3)\n",
        "      labels = labels.reshape(batch_size, song_size , self.label_size)\n",
        "\n",
        "      noise = generate_latent_points(self.noise_size, song_size, batch_size) # generating noise\n",
        "      gen_notes = self.generator.predict([noise, labels])\n",
        "\n",
        "      if(epoch % self.loss_saving_rate == 0):\n",
        "        d_loss_real = self.discriminator.train_on_batch([notes, labels], valid)\n",
        "        d_loss_fake = self.discriminator.train_on_batch([gen_notes, labels], fake)\n",
        "        d_loss = 0.5 * np.add(d_loss_real, d_loss_fake) # 0th for real, 1st for fake\n",
        "        self.discriminator_losses.append((epoch, (d_loss[0], d_loss[1]))) # add discriminator loss\n",
        "\n",
        "      # Train GENERATOR\n",
        "      \n",
        "      # Condition on labels\n",
        "      fake_labels = self.all_normalized_lyrics_embeddings[np.random.randint(0, len(self.all_normalized_lyrics_embeddings), self.song_size * batch_size)].reshape(batch_size, self.song_size, self.label_size) \n",
        "      g_loss = self.combined.train_on_batch([noise, fake_labels], valid) # BCE(D(G(Z)),1))\n",
        "      if(epoch % self.loss_saving_rate == 0):\n",
        "        self.generator_losses.append((epoch, g_loss))  # add generator loss\n",
        "      \n",
        "      # Plot loss\n",
        "      if(epoch % self.loss_plot_rate == 0):\n",
        "        plot_loss(self.generator_losses, self.discriminator_losses)\n",
        "        \n",
        "      #plot distribution\n",
        "      if(epoch % self.distribution_plot_rate == 0):\n",
        "        plot_notes_distribution(epoch, self.generator, self.noise_size, self.grouped_embeddings)\n",
        "        self.save_models(epoch)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9FmT3RKJRC_Y"
      },
      "source": [
        "class CGAN_anylength():\n",
        "  def __init__(self, embeddings, grouped_embeddings, sample_interval, plot_interval):\n",
        "    self.generator_losses = []\n",
        "    self.discriminator_losses = []\n",
        "    self.loss_saving_rate = 1\n",
        "    self.loss_plot_rate = plot_interval\n",
        "    self.distribution_plot_rate = sample_interval\n",
        "\n",
        "    self.all_normalized_lyrics_embeddings = embeddings\n",
        "    self.grouped_embeddings = grouped_embeddings\n",
        "\n",
        "    self.song_size = 20\n",
        "    self.noise_size = 30\n",
        "    self.label_size = 20\n",
        "    self.optimizer = Adam(lr=0.0002, beta_1=0.5)\n",
        "\n",
        "    self.discriminator = self.build_discriminator()\n",
        "    self.discriminator.compile(loss=['binary_crossentropy'],\n",
        "            optimizer=self.optimizer,\n",
        "            metrics=['accuracy'])\n",
        "    self.generator = self.build_generator()\n",
        "\n",
        "    noise = Input(shape = (None,  self.noise_size), name=\"noise\")\n",
        "    labels = Input(shape = (None, self.label_size), name=\"labels\")\n",
        "    generated_midi_sequence = self.generator([noise, labels])\n",
        "\n",
        "    # For the combined model we will only train the generator\n",
        "    self.discriminator.trainable = False\n",
        "    valid = self.discriminator([generated_midi_sequence, labels])\n",
        "\n",
        "    # The combined model  (stacked generator and discriminator)\n",
        "    # Trains generator to fool discriminator\n",
        "    self.combined = keras.Model([noise,labels], valid, name='Combined')\n",
        "    self.combined.compile(loss=['binary_crossentropy'],  # BCE(D(G(Z)),1))\n",
        "        optimizer=self.optimizer)\n",
        "\n",
        "  def build_generator(self):\n",
        "    model = keras.Sequential( [\n",
        "    Dense(400, input_shape=(None, self.noise_size + 20)),\n",
        "    LeakyReLU(alpha=0.2),\n",
        "    LSTM(400, return_sequences=True, activation = \"tanh\"),    \n",
        "    Dropout(0.25),\n",
        "    LSTM(400, return_sequences=True, activation = \"tanh\"),\n",
        "    Dense(3, activation = \"tanh\")\n",
        "    ])\n",
        "\n",
        "    model.build((None, self.noise_size + 20))\n",
        "    model.summary()\n",
        "    \n",
        "    noise  = Input(shape=(self.song_size,  self.noise_size), name='z_input')\n",
        "    labels = Input(shape=(self.song_size, self.label_size), name='class_labels')\n",
        "    \n",
        "    generator_input = Concatenate()([noise, labels])\n",
        "    outputs = model(generator_input)\n",
        "  \n",
        "    return keras.Model([noise, labels], outputs, name='generator')\n",
        "\n",
        "  def build_discriminator(self):\n",
        "    model = keras.Sequential([\n",
        "    LSTM(400, return_sequences=True, \n",
        "         input_shape=(None, self.label_size + 3), activation = \"tanh\"),\n",
        "    Dropout(0.25),\n",
        "    LSTM(400, activation=\"tanh\"),\n",
        "    Flatten(),\n",
        "    Dense(1, activation='sigmoid')])\n",
        "        \n",
        "    model.build((None, self.label_size + 3))\n",
        "    model.summary()\n",
        "\n",
        "    notes  = Input(shape=(self.song_size, 3), name=\"notes\")\n",
        "    labels = Input(shape=(self.song_size, self.label_size), name=\"labels\")\n",
        "    model_input = Concatenate()([notes, labels])\n",
        "\n",
        "    validity = model(model_input)\n",
        "    \n",
        "    return keras.Model([notes,labels], outputs=validity, name='discriminator')\n",
        "\n",
        "  def save_models(self, epoch):\n",
        "    self.generator.save_weights(\"/content/drive/My Drive/Thesis/SecondSemester/CGAN_anylength/generator_epoch_%d.h5\" % epoch)\n",
        "    self.discriminator.save_weights(\"/content/drive/My Drive/Thesis/SecondSemester/CGAN_anylength/discriminator_epoch_%d.h5\" % epoch)\n",
        "    self.combined.save_weights(\"/content/drive/My Drive/Thesis/SecondSemester/CGAN_anylength/combined_epoch_%d.h5\" % epoch)\n",
        "\n",
        "  def train(self, epochs, X_train, y_train,song_size, batch_size=1):\n",
        "    valid = np.ones((batch_size, 1))  # Adversarial ground truths\n",
        "    fake = np.zeros((batch_size, 1))  # Fake labels\n",
        "    for epoch in tqdm_notebook(range(1, epochs)):\n",
        "      # Train DISCRIMINATOR\n",
        "      song_number = np.random.randint(0, len(X_train), batch_size)\n",
        "      notes, labels = X_train[song_number], y_train[song_number]\n",
        "\n",
        "      notes = notes.reshape(batch_size, song_size, 3)\n",
        "      labels = labels.reshape(batch_size, song_size , self.label_size)\n",
        "\n",
        "      noise = generate_latent_points(self.noise_size, song_size, batch_size) # generating noise\n",
        "      gen_notes = self.generator.predict([noise, labels])\n",
        "\n",
        "      if(epoch % self.loss_saving_rate == 0):\n",
        "        d_loss_real = self.discriminator.train_on_batch([notes, labels], valid)\n",
        "        d_loss_fake = self.discriminator.train_on_batch([gen_notes, labels], fake)\n",
        "        d_loss = 0.5 * np.add(d_loss_real, d_loss_fake) # 0th for real, 1st for fake\n",
        "        self.discriminator_losses.append((epoch, (d_loss[0], d_loss[1]))) # add discriminator loss\n",
        "\n",
        "      # Train GENERATOR\n",
        "      \n",
        "      # Condition on labels\n",
        "      fake_labels = self.all_normalized_lyrics_embeddings[np.random.randint(0, len(self.all_normalized_lyrics_embeddings), self.song_size * batch_size)].reshape(batch_size, self.song_size, self.label_size) \n",
        "      g_loss = self.combined.train_on_batch([noise, fake_labels], valid) # BCE(D(G(Z)),1))\n",
        "      if(epoch % self.loss_saving_rate == 0):\n",
        "        self.generator_losses.append((epoch, g_loss))  # add generator loss\n",
        "      \n",
        "      # Plot loss\n",
        "      if(epoch % self.loss_plot_rate == 0):\n",
        "        plot_loss(self.generator_losses, self.discriminator_losses)\n",
        "        \n",
        "      #plot distribution\n",
        "      if(epoch % self.distribution_plot_rate == 0):\n",
        "        plot_notes_distribution(epoch, self.generator, self.noise_size, self.grouped_embeddings)\n",
        "        self.save_models(epoch)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E6L5VjFODm4K",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 232
        },
        "outputId": "2e42a18f-dfc8-4821-a975-62c82efbde3a"
      },
      "source": [
        "class RandomWeightedAverage(_Merge):\n",
        "    \"\"\"Provides a (random) weighted average between real and generated image samples\"\"\"\n",
        "    def _merge_function(self, inputs):\n",
        "        alpha = K.random_uniform((1, 1, 1))\n",
        "        return (alpha * inputs[0]) + ((1 - alpha) * inputs[1])\n",
        "\n",
        "class CWGAN_GP():\n",
        "  def __init__(self, embeddings, grouped_embeddings, sample_interval, plot_interval):\n",
        "    self.generator_losses = []\n",
        "    self.discriminator_losses = []\n",
        "    self.loss_saving_rate = 5\n",
        "    self.loss_plot_rate = plot_interval\n",
        "    self.distribution_plot_rate = sample_interval\n",
        "    self.all_normalized_lyrics_embeddings = embeddings\n",
        "    self.grouped_embeddings = grouped_embeddings\n",
        "\n",
        "    self.song_size = 20\n",
        "    self.noise_size = 100\n",
        "    self.label_size = 20\n",
        "    self.optimizer = RMSprop(lr=0.00005)\n",
        "    self.n_critic = 5\n",
        "    \n",
        "    # Build the generator and critic\n",
        "    self.generator = self.build_generator()\n",
        "    self.critic = self.build_critic()\n",
        "\n",
        "    #-------------------------------\n",
        "    # Construct Computational Graph\n",
        "    #       for the Critic\n",
        "    #-------------------------------\n",
        "\n",
        "    # Freeze generator's layers while training critic\n",
        "    self.generator.trainable = False\n",
        "\n",
        "    # Song input (real sample)\n",
        "    real_song = Input(shape=(self.song_size,  3))\n",
        "\n",
        "    # Noise input\n",
        "    noise = Input(shape=(self.song_size,  self.noise_size))\n",
        "    # Generate sequence based of noise (fake sample) and add labels to the input \n",
        "    labels = Input(shape = (self.song_size, self.label_size), name=\"labels\")\n",
        "    fake_song = self.generator([noise, labels])\n",
        "\n",
        "    # Discriminator determines validity of the real and fake images\n",
        "    fake = self.critic([fake_song, labels])\n",
        "    valid = self.critic([real_song, labels])\n",
        "\n",
        "    # Construct weighted average between real and fake sequences\n",
        "    interpolated_song = RandomWeightedAverage()([real_song, fake_song])\n",
        "    \n",
        "    # Determine validity of weighted sample\n",
        "    validity_interpolated = self.critic([interpolated_song, labels])\n",
        "\n",
        "    # Use Python partial to provide loss function with additional\n",
        "    # 'averaged_samples' argument\n",
        "    partial_gp_loss = partial(self.gradient_penalty_loss,\n",
        "                      averaged_samples=interpolated_song)\n",
        "    partial_gp_loss.__name__ = 'gradient_penalty' # Keras requires function names\n",
        "\n",
        "    self.critic_model = Model(inputs=[real_song, labels, noise], outputs=[valid, fake, validity_interpolated])\n",
        "    self.critic_model.compile(loss=[self.wasserstein_loss,\n",
        "                                    self.wasserstein_loss,\n",
        "                                    partial_gp_loss],\n",
        "                                    optimizer=self.optimizer,\n",
        "                                    loss_weights=[1, 1, 10])\n",
        "    #-------------------------------\n",
        "    # Construct Computational Graph\n",
        "    #         for Generator\n",
        "    #-------------------------------\n",
        "\n",
        "    # For the generator we freeze the critic's layers\n",
        "    self.critic.trainable = False\n",
        "    self.generator.trainable = True\n",
        "\n",
        "    # Sampled noise for input to generator\n",
        "    noise = Input(shape=(self.song_size,  self.noise_size))\n",
        "    # add label to the input\n",
        "    labels = Input(shape = (self.song_size, self.label_size), name=\"labels\")\n",
        "    # Generate song based of noise\n",
        "    fake_song = self.generator([noise, labels])\n",
        "    # Discriminator determines validity\n",
        "    valid = self.critic([fake_song, labels])\n",
        "    # Defines generator model\n",
        "    self.generator_model = Model([noise, labels], valid)\n",
        "    self.generator_model.compile(loss=self.wasserstein_loss, optimizer=self.optimizer)\n",
        "\n",
        "  def build_generator(self):\n",
        "    model = keras.Sequential([\n",
        "    Dense(400, input_shape=(self.song_size, self.noise_size + 20)),\n",
        "    LeakyReLU(alpha=0.2),\n",
        "    LSTM(400, return_sequences=True, activation = \"tanh\",  unroll=True),    \n",
        "    LSTM(400, return_sequences=True, activation = \"tanh\",  unroll=True),\n",
        "    Dense(3, activation = \"tanh\")\n",
        "    ])\n",
        "    \n",
        "    noise  = Input(shape=(self.song_size,  self.noise_size), name='z_input')\n",
        "    labels = Input(shape=(self.song_size, self.label_size), name='class_labels')\n",
        "    generator_input = Concatenate()([noise, labels])\n",
        "    outputs = model(generator_input)\n",
        "  \n",
        "    return keras.Model([noise, labels], outputs, name='generator')\n",
        "\n",
        "  def gradient_penalty_loss(self, y_true, y_pred, averaged_samples):\n",
        "    \"\"\"\n",
        "    Computes gradient penalty based on prediction and weighted real / fake samples\n",
        "    \"\"\"\n",
        "    gradients = K.gradients(y_pred, averaged_samples)[0]\n",
        "    # compute the euclidean norm by squaring ...\n",
        "    gradients_sqr = K.square(gradients)\n",
        "    #   ... summing over the rows ...\n",
        "    gradients_sqr_sum = K.sum(gradients_sqr,\n",
        "                              axis=np.arange(1, len(gradients_sqr.shape)))\n",
        "    #   ... and sqrt\n",
        "    gradient_l2_norm = K.sqrt(gradients_sqr_sum)\n",
        "    # compute lambda * (1 - ||grad||)^2 still for each single sample\n",
        "    gradient_penalty = K.square(1 - gradient_l2_norm)\n",
        "    # return the mean as loss over all the batch samples\n",
        "    return K.mean(gradient_penalty)\n",
        "\n",
        "\n",
        "  def wasserstein_loss(self, y_true, y_pred):\n",
        "    return K.mean(y_true * y_pred)\n",
        "\n",
        "  def save_models(self, epoch):\n",
        "    self.generator.save_weights(\"/content/drive/My Drive/Thesis/SecondSemester/CWGAN_GP/generator_epoch_%d.h5\" % epoch)\n",
        "    self.critic.save_weights(\"/content/drive/My Drive/Thesis/SecondSemester/CWGAN_GP/critic_epoch_%d.h5\" % epoch)\n",
        "    self.critic_model.save_weights(\"/content/drive/My Drive/Thesis/SecondSemester/CWGAN_GP/critic_model_epoch_%d.h5\" % epoch)\n",
        "    self.generator_model.save_weights(\"/content/drive/My Drive/Thesis/SecondSemester/CWGAN_GP/generator_model_epoch_%d.h5\" % epoch)\n",
        "  def build_critic(self):\n",
        "    model = keras.Sequential([\n",
        "    LSTM(400, return_sequences=True, input_shape=(self.song_size, self.label_size + 3), activation = \"tanh\", unroll=True),\n",
        "    LSTM(400, activation=\"tanh\",  unroll=True),\n",
        "    Dense(1)])\n",
        "\n",
        "    notes  = Input(shape=(self.song_size, 3))\n",
        "    labels = Input(shape=(self.song_size, self.label_size))\n",
        "    model_input = Concatenate()([notes, labels])\n",
        "    validity = model(model_input)\n",
        "    \n",
        "    return keras.Model([notes,labels], outputs=validity, name='discriminator')\n",
        "\n",
        "  def train(self, epochs, X_train, y_train, song_size, batch_size=16):\n",
        "    valid = -np.ones((batch_size, 1))\n",
        "    fake = np.ones((batch_size, 1))\n",
        "    dummy = np.zeros((batch_size, 1)) # Dummy gt for gradient penalty\n",
        "\n",
        "    for epoch in tqdm_notebook(range(1, epochs)):\n",
        "      for _ in range(self.n_critic):\n",
        "         # ---------------------\n",
        "         #  Train Critic\n",
        "         # ---------------------\n",
        "         idx = np.random.randint(0, len(X_train), batch_size)\n",
        "\n",
        "         notes = X_train[idx].reshape(batch_size, song_size, 3)\n",
        "         labels = y_train[idx].reshape(batch_size, song_size , self.label_size)\n",
        "         \n",
        "         noise = generate_latent_points(self.noise_size, song_size, batch_size) # generating noise\n",
        "         d_loss = self.critic_model.train_on_batch([notes, labels, noise], [valid, fake, dummy])\n",
        "\n",
        "        \n",
        "     \n",
        "      # ---------------------\n",
        "      #  Train Generator\n",
        "      # ---------------------\n",
        "     \n",
        "      noise = generate_latent_points(self.noise_size, song_size, batch_size) # generating noise\n",
        "      fake_labels = self.all_normalized_lyrics_embeddings[np.random.randint(0, len(self.all_normalized_lyrics_embeddings), self.song_size * batch_size)].reshape(batch_size, self.song_size, self.label_size) \n",
        "      g_loss = self.generator_model.train_on_batch([noise, fake_labels], valid)\n",
        "\n",
        "      if(epoch % self.loss_saving_rate == 0):\n",
        "        self.generator_losses.append((epoch, g_loss))  # add generator loss\n",
        "        self.discriminator_losses.append((epoch, (d_loss[0], d_loss[1]))) # add discriminator loss\n",
        "      # Plot distribution\n",
        "      if(epoch % self.distribution_plot_rate == 0):\n",
        "        plot_notes_distribution(epoch, self.generator, self.noise_size, self.grouped_embeddings)\n",
        "        self.save_models(epoch)\n",
        "      # Plot loss\n",
        "      if(epoch % self.loss_plot_rate == 0):\n",
        "        plot_loss(self.generator_losses, self.discriminator_losses)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-3-cf1652306dfb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mclass\u001b[0m \u001b[0mRandomWeightedAverage\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_Merge\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m     \u001b[0;34m\"\"\"Provides a (random) weighted average between real and generated image samples\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_merge_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m         \u001b[0;32mglobal\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0malpha\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mK\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom_uniform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name '_Merge' is not defined"
          ]
        }
      ]
    }
  ]
}